{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 女子高校生対象AI活用型データサイエンス教育：統計分析\n",
    "\n",
    "本ノートブックでは、以下の分析を実施します：\n",
    "1. データの読み込みと前処理\n",
    "2. 自己効力感の事前・事後比較（Wilcoxon検定、FDR補正）\n",
    "3. 効果量の算出（paired Cohen's d）\n",
    "4. 事後アンケートの記述統計\n",
    "5. 可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定\n",
    "try:\n",
    "    import japanize_matplotlib\n",
    "    print('✅ 日本語フォント設定完了')\n",
    "except ImportError:\n",
    "    print('⚠️ japanize-matplotlib をインストールしてください')\n",
    "\n",
    "# 表示設定\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルパス（環境に合わせて変更）\n",
    "BEFORE_SURVEY_FILE = 'data/sample_before.csv'\n",
    "AFTER_SURVEY_FILE = 'data/sample_after.csv'\n",
    "\n",
    "# 共通質問項目（自己効力感4項目）\n",
    "COMMON_QUESTIONS = [\n",
    "    'データを表やグラフに整理できる自信がある。',\n",
    "    'グラフから特徴や傾向を読み取れる自信がある。',\n",
    "    'プログラムを実行し結果を解釈できる自信がある。',\n",
    "    '身近なテーマでデータ分析課題を立てられる自信がある。'\n",
    "]\n",
    "\n",
    "# 短縮ラベル\n",
    "SHORT_LABELS = ['Q1 整理', 'Q2 読解', 'Q3 プログラム', 'Q4 課題設定']\n",
    "\n",
    "# ID列\n",
    "ID_COLUMN = '匿名コード（8桁の半角英数字）'\n",
    "EXCLUDE_COLUMNS = ['タイムスタンプ', ID_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 統計関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d_paired(before, after):\n",
    "    \"\"\"対応のある効果量（paired d）を算出\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    before : array-like\n",
    "        事前スコア\n",
    "    after : array-like\n",
    "        事後スコア\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        効果量d（差分の平均 / 差分の標準偏差）\n",
    "    \"\"\"\n",
    "    diff = np.array(after) - np.array(before)\n",
    "    diff = diff[~np.isnan(diff)]\n",
    "    if len(diff) < 2:\n",
    "        return np.nan\n",
    "    return diff.mean() / diff.std(ddof=1)\n",
    "\n",
    "\n",
    "def benjamini_hochberg_fdr(p_values):\n",
    "    \"\"\"Benjamini-Hochberg法によるFDR補正\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_values : array-like\n",
    "        p値の配列（NaNを含んでも可）\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        FDR補正後のq値\n",
    "    \"\"\"\n",
    "    p = np.array(p_values, dtype=float)\n",
    "    n = np.sum(~np.isnan(p))\n",
    "    q = np.full_like(p, np.nan)\n",
    "    \n",
    "    if n == 0:\n",
    "        return q\n",
    "    \n",
    "    # NaNでない値のインデックスを取得\n",
    "    valid_idx = np.where(~np.isnan(p))[0]\n",
    "    valid_p = p[valid_idx]\n",
    "    \n",
    "    # ソート\n",
    "    sorted_idx = np.argsort(valid_p)\n",
    "    sorted_p = valid_p[sorted_idx]\n",
    "    ranks = np.arange(1, n + 1)\n",
    "    \n",
    "    # q値の計算\n",
    "    q_sorted = sorted_p * n / ranks\n",
    "    \n",
    "    # 単調性の確保（後ろから累積最小値）\n",
    "    q_sorted = np.minimum.accumulate(q_sorted[::-1])[::-1]\n",
    "    q_sorted = np.clip(q_sorted, 0, 1)\n",
    "    \n",
    "    # 元の順序に戻す\n",
    "    q[valid_idx[sorted_idx]] = q_sorted\n",
    "    \n",
    "    return q\n",
    "\n",
    "\n",
    "def interpret_d(d):\n",
    "    \"\"\"効果量dの解釈（Cohen, 1988）\"\"\"\n",
    "    if np.isnan(d):\n",
    "        return '-'\n",
    "    abs_d = abs(d)\n",
    "    if abs_d < 0.2:\n",
    "        return 'ごく小'\n",
    "    elif abs_d < 0.5:\n",
    "        return '小'\n",
    "    elif abs_d < 0.8:\n",
    "        return '中'\n",
    "    else:\n",
    "        return '大'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(filepath, encodings=['utf-8-sig', 'utf-8', 'shift-jis', 'cp932']):\n",
    "    \"\"\"CSVファイルを読み込み、重複を除去\"\"\"\n",
    "    df = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding=enc)\n",
    "            print(f'✅ {filepath} を {enc} で読み込みました')\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if df is None:\n",
    "        raise Exception(f'ファイルを読み込めません: {filepath}')\n",
    "    \n",
    "    # 重複処理（タイムスタンプが最新のものを残す）\n",
    "    if 'タイムスタンプ' in df.columns and ID_COLUMN in df.columns:\n",
    "        df['タイムスタンプ'] = pd.to_datetime(df['タイムスタンプ'])\n",
    "        df = df.sort_values(by=[ID_COLUMN, 'タイムスタンプ'], ascending=[True, False])\n",
    "        df = df.drop_duplicates(subset=[ID_COLUMN], keep='first')\n",
    "        print(f'   重複除去後: {len(df)}件')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# データ読み込み（ファイルがある場合のみ実行）\n",
    "# df_before = load_and_preprocess(BEFORE_SURVEY_FILE)\n",
    "# df_after = load_and_preprocess(AFTER_SURVEY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 事前・事後の対応データ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paired_data(df_before, df_after, questions, id_col):\n",
    "    \"\"\"事前・事後の対応データを作成\"\"\"\n",
    "    \n",
    "    # 必要な列を抽出\n",
    "    cols = [id_col] + questions\n",
    "    df_b = df_before[cols].copy()\n",
    "    df_a = df_after[cols].copy()\n",
    "    \n",
    "    # 数値変換\n",
    "    for q in questions:\n",
    "        df_b[q] = pd.to_numeric(df_b[q], errors='coerce')\n",
    "        df_a[q] = pd.to_numeric(df_a[q], errors='coerce')\n",
    "    \n",
    "    # リネーム\n",
    "    df_b = df_b.rename(columns={q: f'{q}_before' for q in questions})\n",
    "    df_a = df_a.rename(columns={q: f'{q}_after' for q in questions})\n",
    "    \n",
    "    # マージ\n",
    "    df_merged = pd.merge(df_b, df_a, on=id_col, how='inner')\n",
    "    print(f'対応データ: {len(df_merged)}名')\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# df_merged = create_paired_data(df_before, df_after, COMMON_QUESTIONS, ID_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 統計検定の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_paired_tests(df_merged, questions, labels=None):\n",
    "    \"\"\"対応のある検定を実施し、結果を表形式で返す\"\"\"\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [f'Q{i+1}' for i in range(len(questions))]\n",
    "    \n",
    "    results = []\n",
    "    p_values = []\n",
    "    \n",
    "    for q, label in zip(questions, labels):\n",
    "        before = df_merged[f'{q}_before'].dropna()\n",
    "        after = df_merged[f'{q}_after'].dropna()\n",
    "        \n",
    "        # 両方に値がある行のみ\n",
    "        mask = ~(df_merged[f'{q}_before'].isna() | df_merged[f'{q}_after'].isna())\n",
    "        b = df_merged.loc[mask, f'{q}_before']\n",
    "        a = df_merged.loc[mask, f'{q}_after']\n",
    "        n = len(b)\n",
    "        \n",
    "        if n < 2:\n",
    "            results.append({\n",
    "                '項目': label,\n",
    "                'n': n,\n",
    "                'M前(SD)': '-',\n",
    "                'M後(SD)': '-',\n",
    "                '変化': '-',\n",
    "                'd': '-',\n",
    "                't_p': np.nan,\n",
    "                'wilcoxon_p': np.nan\n",
    "            })\n",
    "            p_values.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        # 統計量の計算\n",
    "        mean_b, std_b = b.mean(), b.std()\n",
    "        mean_a, std_a = a.mean(), a.std()\n",
    "        change = mean_a - mean_b\n",
    "        d = cohens_d_paired(b, a)\n",
    "        \n",
    "        # t検定\n",
    "        t_stat, t_p = stats.ttest_rel(a, b)\n",
    "        \n",
    "        # Wilcoxon検定\n",
    "        try:\n",
    "            w_stat, w_p = stats.wilcoxon(a - b, alternative='two-sided')\n",
    "        except ValueError:\n",
    "            w_p = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            '項目': label,\n",
    "            'n': n,\n",
    "            'M前(SD)': f'{mean_b:.2f} ({std_b:.2f})',\n",
    "            'M後(SD)': f'{mean_a:.2f} ({std_a:.2f})',\n",
    "            '変化': f'{change:+.2f}',\n",
    "            'd': f'{d:.2f}',\n",
    "            't_p': t_p,\n",
    "            'wilcoxon_p': w_p\n",
    "        })\n",
    "        p_values.append(w_p)\n",
    "    \n",
    "    # FDR補正\n",
    "    q_values = benjamini_hochberg_fdr(p_values)\n",
    "    \n",
    "    for i, q_val in enumerate(q_values):\n",
    "        if np.isnan(q_val):\n",
    "            results[i]['wilcoxon_q'] = '-'\n",
    "        else:\n",
    "            sig = '**' if q_val < 0.01 else '*' if q_val < 0.05 else ''\n",
    "            results[i]['wilcoxon_q'] = f'{q_val:.3f}{sig}'\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 実行例（データがある場合）\n",
    "# results_df = run_paired_tests(df_merged, COMMON_QUESTIONS, SHORT_LABELS)\n",
    "# print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pre_post_scatter(df_merged, question, title, save_path=None):\n",
    "    \"\"\"事前・事後の散布図を作成\"\"\"\n",
    "    \n",
    "    before_col = f'{question}_before'\n",
    "    after_col = f'{question}_after'\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # 散布図（ジッター付き）\n",
    "    sns.regplot(\n",
    "        x=df_merged[before_col], \n",
    "        y=df_merged[after_col],\n",
    "        fit_reg=False, \n",
    "        x_jitter=0.2, \n",
    "        y_jitter=0.2,\n",
    "        scatter_kws={'alpha': 0.6, 's': 100}\n",
    "    )\n",
    "    \n",
    "    # 対角線（変化なし）\n",
    "    plt.plot([1, 5], [1, 5], 'r--', linewidth=2, label='変化なし')\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('事前の回答 (1:自信がない 〜 5:自信がある)', fontsize=12)\n",
    "    plt.ylabel('事後の回答 (1:自信がない 〜 5:自信がある)', fontsize=12)\n",
    "    plt.xticks(range(1, 6))\n",
    "    plt.yticks(range(1, 6))\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axis('square')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f'✅ 保存: {save_path}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bar_chart(data, title, xlabel='回答', ylabel='回答者数', save_path=None):\n",
    "    \"\"\"棒グラフを作成\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    ax = sns.countplot(data=data, x='label', hue='label', palette='viridis', legend=False)\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    \n",
    "    # 数値ラベル\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(\n",
    "            f'{int(p.get_height())}',\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "            ha='center', va='center', \n",
    "            xytext=(0, 5), textcoords='offset points'\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f'✅ 保存: {save_path}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ワードクラウド生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(texts, font_path=None, save_path=None):\n",
    "    \"\"\"ワードクラウドを生成\"\"\"\n",
    "    \n",
    "    # テキストの結合\n",
    "    text_data = ' '.join(texts.dropna().astype(str).tolist())\n",
    "    \n",
    "    if not text_data.strip():\n",
    "        print('⚠️ テキストデータがありません')\n",
    "        return\n",
    "    \n",
    "    # ストップワード\n",
    "    stopwords = {' ', 'こと', 'もの', 'ため', 'これ', 'それ', 'ある', 'いる', 'する', 'なる', 'できる'}\n",
    "    \n",
    "    # ワードクラウド生成\n",
    "    wc_kwargs = {\n",
    "        'width': 800,\n",
    "        'height': 400,\n",
    "        'background_color': 'white',\n",
    "        'stopwords': stopwords,\n",
    "        'collocations': False\n",
    "    }\n",
    "    \n",
    "    if font_path:\n",
    "        wc_kwargs['font_path'] = font_path\n",
    "    \n",
    "    wc = WordCloud(**wc_kwargs).generate(text_data)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('自由記述のワードクラウド', fontsize=14)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f'✅ 保存: {save_path}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 内的整合性（Cronbach's α）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cronbachs_alpha(df, items):\n",
    "    \"\"\"Cronbach's αを算出\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        データフレーム\n",
    "    items : list\n",
    "        項目名のリスト\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cronbach's α\n",
    "    \"\"\"\n",
    "    item_data = df[items].dropna()\n",
    "    n_items = len(items)\n",
    "    \n",
    "    if len(item_data) < 2 or n_items < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # 各項目の分散\n",
    "    item_vars = item_data.var(ddof=1)\n",
    "    sum_item_vars = item_vars.sum()\n",
    "    \n",
    "    # 合計点の分散\n",
    "    total_var = item_data.sum(axis=1).var(ddof=1)\n",
    "    \n",
    "    # Cronbach's α\n",
    "    alpha = (n_items / (n_items - 1)) * (1 - sum_item_vars / total_var)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "# 使用例\n",
    "# alpha_before = cronbachs_alpha(df_before, COMMON_QUESTIONS)\n",
    "# alpha_after = cronbachs_alpha(df_after, COMMON_QUESTIONS)\n",
    "# print(f'事前α = {alpha_before:.2f}, 事後α = {alpha_after:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 実行例（サンプルデータ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータで動作確認\n",
    "np.random.seed(42)\n",
    "n = 38\n",
    "\n",
    "# サンプルデータ生成（実際のデータに近い分布）\n",
    "sample_data = pd.DataFrame({\n",
    "    'Q1_before': np.random.choice([1,2,3,4,5], n, p=[0.15, 0.35, 0.30, 0.15, 0.05]),\n",
    "    'Q1_after': np.random.choice([1,2,3,4,5], n, p=[0.05, 0.15, 0.35, 0.30, 0.15]),\n",
    "    'Q2_before': np.random.choice([1,2,3,4,5], n, p=[0.05, 0.20, 0.35, 0.30, 0.10]),\n",
    "    'Q2_after': np.random.choice([1,2,3,4,5], n, p=[0.05, 0.15, 0.35, 0.30, 0.15]),\n",
    "})\n",
    "\n",
    "# 効果量の計算例\n",
    "d_q1 = cohens_d_paired(sample_data['Q1_before'], sample_data['Q1_after'])\n",
    "d_q2 = cohens_d_paired(sample_data['Q2_before'], sample_data['Q2_after'])\n",
    "\n",
    "print(f'Q1の効果量 d = {d_q1:.2f} ({interpret_d(d_q1)})')\n",
    "print(f'Q2の効果量 d = {d_q2:.2f} ({interpret_d(d_q2)})')\n",
    "\n",
    "# Wilcoxon検定\n",
    "_, p1 = stats.wilcoxon(sample_data['Q1_after'] - sample_data['Q1_before'])\n",
    "_, p2 = stats.wilcoxon(sample_data['Q2_after'] - sample_data['Q2_before'])\n",
    "\n",
    "# FDR補正\n",
    "q_values = benjamini_hochberg_fdr([p1, p2])\n",
    "\n",
    "print(f'\\nWilcoxon検定（FDR補正後）:')\n",
    "print(f'  Q1: p = {p1:.4f}, q = {q_values[0]:.4f}')\n",
    "print(f'  Q2: p = {p2:.4f}, q = {q_values[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "- Bandura, A. (1977). Self-efficacy: Toward a unifying theory of behavioral change. *Psychological Review*, 84(2), 191-215.\n",
    "- Cohen, J. (1988). *Statistical power analysis for the behavioral sciences* (2nd ed.). Lawrence Erlbaum Associates.\n",
    "- Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. *Journal of the Royal Statistical Society: Series B*, 57(1), 289-300."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
